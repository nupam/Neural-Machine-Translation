{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Neural Machine Translation\n",
    "\n",
    "This translation model has been heavily inspired from coursera, Deeplearing.ai, course on Deep learning specialisation.\n",
    "\n",
    "The included utility file, nmt_utils ha been include as is as provide in the coourse\n",
    " \n",
    "Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\"). You will do this using an attention model, one of the most sophisticated sequence to sequence models. \n",
    "requires keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/etc/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will train the model on a dataset of 10000 human readable dates and their equivalent, standardized, machine readable dates. Let's run the following cells to load the dataset and print some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 27841.05it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " '/': 2,\n",
       " '0': 3,\n",
       " '1': 4,\n",
       " '2': 5,\n",
       " '3': 6,\n",
       " '4': 7,\n",
       " '5': 8,\n",
       " '6': 9,\n",
       " '7': 10,\n",
       " '8': 11,\n",
       " '9': 12,\n",
       " 'a': 13,\n",
       " 'b': 14,\n",
       " 'c': 15,\n",
       " 'd': 16,\n",
       " 'e': 17,\n",
       " 'f': 18,\n",
       " 'g': 19,\n",
       " 'h': 20,\n",
       " 'i': 21,\n",
       " 'j': 22,\n",
       " 'l': 23,\n",
       " 'm': 24,\n",
       " 'n': 25,\n",
       " 'o': 26,\n",
       " 'p': 27,\n",
       " 'r': 28,\n",
       " 's': 29,\n",
       " 't': 30,\n",
       " 'u': 31,\n",
       " 'v': 32,\n",
       " 'w': 33,\n",
       " 'y': 34,\n",
       " '<unk>': 35,\n",
       " '<pad>': 36}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]\n",
    "human_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we've loaded:\n",
    "- `dataset`: a list of tuples of (human readable date, machine readable date)\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index \n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. These indices are not necessarily consistent with `human_vocab`. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n",
    "\n",
    "Let's preprocess the data and map the raw text data into the index values. We will also use Tx=30 (which we assume is the maximum length of the human readable date; if we get a longer input, we would have to truncate it) and Ty=10 (since \"YYYY-MM-DD\" is 10 characters long). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have:\n",
    "- `X`: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via `human_vocab`. Each date is further padded to $T_x$ values with a special character (< pad >). `X.shape = (m, Tx)`\n",
    "- `Y`: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in `machine_vocab`. You should have `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`, the \"1\" entry's index is mapped to the character thanks to `human_vocab`. `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`, the \"1\" entry's index is mapped to the character thanks to `machine_vocab`. `Yoh.shape = (m, Tx, len(machine_vocab))`. Here, `len(machine_vocab) = 11` since there are 11 characters ('-' as well as 0-9). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also look at some examples of preprocessed training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 9 may 1998\n",
      "Target date: 1998-05-09\n",
      "\n",
      "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#description of the attention model has been provided int the root of the repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    s_prev = repeator(s_prev)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    e = densor1(concat)\n",
    "    energies = densor2(e)\n",
    "    alphas = activator(energies)\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "   \n",
    "    # Step 1: Define your pre-attention Bi-LSTM\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        \n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        s, _, c = post_activation_LSTM_cell(initial_state = [s, c], inputs = context)\n",
    "        out = output_layer(s)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    model = Model(inputs=[X, s0, c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a summary of the model to check if it matches the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#create an optimizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(optimizer=opt,\n",
    "                    metrics=['accuracy'],\n",
    "                    loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining inputs and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit the model and run it for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 5s 459us/step - loss: 7.6239 - dense_3_loss: 2.0144 - dense_3_acc: 0.9732 - dense_3_acc_1: 0.9774 - dense_3_acc_2: 0.6016 - dense_3_acc_3: 0.2948 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9462 - dense_3_acc_6: 0.5247 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.5646 - dense_3_acc_9: 0.2539\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 5s 467us/step - loss: 6.7051 - dense_3_loss: 1.8549 - dense_3_acc: 0.9791 - dense_3_acc_1: 0.9796 - dense_3_acc_2: 0.6425 - dense_3_acc_3: 0.3791 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9616 - dense_3_acc_6: 0.6247 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.6327 - dense_3_acc_9: 0.3056\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 5s 467us/step - loss: 5.9269 - dense_3_loss: 1.6735 - dense_3_acc: 0.9798 - dense_3_acc_1: 0.9816 - dense_3_acc_2: 0.6963 - dense_3_acc_3: 0.4430 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9687 - dense_3_acc_6: 0.7123 - dense_3_acc_7: 0.9999 - dense_3_acc_8: 0.6742 - dense_3_acc_9: 0.3642\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 5s 468us/step - loss: 5.2328 - dense_3_loss: 1.4818 - dense_3_acc: 0.9833 - dense_3_acc_1: 0.9842 - dense_3_acc_2: 0.7379 - dense_3_acc_3: 0.5113 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9728 - dense_3_acc_6: 0.7492 - dense_3_acc_7: 0.9999 - dense_3_acc_8: 0.7261 - dense_3_acc_9: 0.4361\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 5s 475us/step - loss: 4.6247 - dense_3_loss: 1.3027 - dense_3_acc: 0.9853 - dense_3_acc_1: 0.9854 - dense_3_acc_2: 0.7657 - dense_3_acc_3: 0.5843 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9779 - dense_3_acc_6: 0.7767 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.7497 - dense_3_acc_9: 0.5014\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 5s 462us/step - loss: 4.1586 - dense_3_loss: 1.1568 - dense_3_acc: 0.9857 - dense_3_acc_1: 0.9856 - dense_3_acc_2: 0.7946 - dense_3_acc_3: 0.6509 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9795 - dense_3_acc_6: 0.7989 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.7728 - dense_3_acc_9: 0.5607\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 5s 495us/step - loss: 3.7645 - dense_3_loss: 1.0234 - dense_3_acc: 0.9875 - dense_3_acc_1: 0.9884 - dense_3_acc_2: 0.8123 - dense_3_acc_3: 0.7130 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9815 - dense_3_acc_6: 0.8099 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.7899 - dense_3_acc_9: 0.6168\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 6s 560us/step - loss: 3.4491 - dense_3_loss: 0.9225 - dense_3_acc: 0.9877 - dense_3_acc_1: 0.9890 - dense_3_acc_2: 0.8289 - dense_3_acc_3: 0.7682 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9811 - dense_3_acc_6: 0.8218 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8028 - dense_3_acc_9: 0.6616\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 6s 560us/step - loss: 3.1727 - dense_3_loss: 0.8428 - dense_3_acc: 0.9885 - dense_3_acc_1: 0.9896 - dense_3_acc_2: 0.8431 - dense_3_acc_3: 0.8162 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9815 - dense_3_acc_6: 0.8316 - dense_3_acc_7: 0.9998 - dense_3_acc_8: 0.8126 - dense_3_acc_9: 0.6984\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 8s 771us/step - loss: 2.9378 - dense_3_loss: 0.7669 - dense_3_acc: 0.9887 - dense_3_acc_1: 0.9896 - dense_3_acc_2: 0.8543 - dense_3_acc_3: 0.8475 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9827 - dense_3_acc_6: 0.8354 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8189 - dense_3_acc_9: 0.7318\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 5s 462us/step - loss: 2.7307 - dense_3_loss: 0.7053 - dense_3_acc: 0.9901 - dense_3_acc_1: 0.9907 - dense_3_acc_2: 0.8668 - dense_3_acc_3: 0.8651 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9837 - dense_3_acc_6: 0.8459 - dense_3_acc_7: 0.9999 - dense_3_acc_8: 0.8264 - dense_3_acc_9: 0.7592\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 5s 479us/step - loss: 2.5463 - dense_3_loss: 0.6502 - dense_3_acc: 0.9909 - dense_3_acc_1: 0.9914 - dense_3_acc_2: 0.8728 - dense_3_acc_3: 0.8912 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9838 - dense_3_acc_6: 0.8531 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8296 - dense_3_acc_9: 0.7821\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 5s 476us/step - loss: 2.3986 - dense_3_loss: 0.6066 - dense_3_acc: 0.9904 - dense_3_acc_1: 0.9923 - dense_3_acc_2: 0.8768 - dense_3_acc_3: 0.8997 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9846 - dense_3_acc_6: 0.8627 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8341 - dense_3_acc_9: 0.7995\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 5s 539us/step - loss: 2.2552 - dense_3_loss: 0.5684 - dense_3_acc: 0.9915 - dense_3_acc_1: 0.9923 - dense_3_acc_2: 0.8796 - dense_3_acc_3: 0.9142 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9854 - dense_3_acc_6: 0.8687 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8419 - dense_3_acc_9: 0.8145\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 5s 505us/step - loss: 2.1386 - dense_3_loss: 0.5321 - dense_3_acc: 0.9920 - dense_3_acc_1: 0.9923 - dense_3_acc_2: 0.8845 - dense_3_acc_3: 0.9196 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9848 - dense_3_acc_6: 0.8732 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8456 - dense_3_acc_9: 0.8251\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 5s 486us/step - loss: 2.0404 - dense_3_loss: 0.5063 - dense_3_acc: 0.9923 - dense_3_acc_1: 0.9934 - dense_3_acc_2: 0.8877 - dense_3_acc_3: 0.9252 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9862 - dense_3_acc_6: 0.8800 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8523 - dense_3_acc_9: 0.8357\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 5s 492us/step - loss: 1.9511 - dense_3_loss: 0.4819 - dense_3_acc: 0.9929 - dense_3_acc_1: 0.9935 - dense_3_acc_2: 0.8903 - dense_3_acc_3: 0.9333 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9851 - dense_3_acc_6: 0.8844 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8551 - dense_3_acc_9: 0.8445\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 5s 483us/step - loss: 1.8627 - dense_3_loss: 0.4572 - dense_3_acc: 0.9935 - dense_3_acc_1: 0.9942 - dense_3_acc_2: 0.8927 - dense_3_acc_3: 0.9369 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9857 - dense_3_acc_6: 0.8902 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8591 - dense_3_acc_9: 0.8547\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 5s 481us/step - loss: 1.7844 - dense_3_loss: 0.4380 - dense_3_acc: 0.9941 - dense_3_acc_1: 0.9943 - dense_3_acc_2: 0.8949 - dense_3_acc_3: 0.9409 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9873 - dense_3_acc_6: 0.8969 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8661 - dense_3_acc_9: 0.8603\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 5s 500us/step - loss: 1.7193 - dense_3_loss: 0.4212 - dense_3_acc: 0.9946 - dense_3_acc_1: 0.9948 - dense_3_acc_2: 0.8965 - dense_3_acc_3: 0.9426 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.9878 - dense_3_acc_6: 0.8995 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8669 - dense_3_acc_9: 0.8639\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 5s 506us/step - loss: 1.6566 - dense_3_loss: 0.4038 - dense_3_acc: 0.9953 - dense_3_acc_1: 0.9946 - dense_3_acc_2: 0.8985 - dense_3_acc_3: 0.9456 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9881 - dense_3_acc_6: 0.9032 - dense_3_acc_7: 0.9999 - dense_3_acc_8: 0.8717 - dense_3_acc_9: 0.8715\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 5s 498us/step - loss: 1.5997 - dense_3_loss: 0.3912 - dense_3_acc: 0.9957 - dense_3_acc_1: 0.9959 - dense_3_acc_2: 0.9011 - dense_3_acc_3: 0.9473 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9873 - dense_3_acc_6: 0.9064 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8752 - dense_3_acc_9: 0.8732\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 5s 503us/step - loss: 1.5520 - dense_3_loss: 0.3783 - dense_3_acc: 0.9960 - dense_3_acc_1: 0.9957 - dense_3_acc_2: 0.9052 - dense_3_acc_3: 0.9461 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9876 - dense_3_acc_6: 0.9095 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8779 - dense_3_acc_9: 0.8776\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 523us/step - loss: 1.5051 - dense_3_loss: 0.3676 - dense_3_acc: 0.9964 - dense_3_acc_1: 0.9962 - dense_3_acc_2: 0.9099 - dense_3_acc_3: 0.9457 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9889 - dense_3_acc_6: 0.9126 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8813 - dense_3_acc_9: 0.8800\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 5s 505us/step - loss: 1.4636 - dense_3_loss: 0.3593 - dense_3_acc: 0.9963 - dense_3_acc_1: 0.9963 - dense_3_acc_2: 0.9108 - dense_3_acc_3: 0.9478 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9881 - dense_3_acc_6: 0.9181 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8842 - dense_3_acc_9: 0.8815\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 5s 488us/step - loss: 1.4186 - dense_3_loss: 0.3497 - dense_3_acc: 0.9973 - dense_3_acc_1: 0.9970 - dense_3_acc_2: 0.9141 - dense_3_acc_3: 0.9465 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9888 - dense_3_acc_6: 0.9195 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8888 - dense_3_acc_9: 0.8821\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 5s 490us/step - loss: 1.3826 - dense_3_loss: 0.3420 - dense_3_acc: 0.9971 - dense_3_acc_1: 0.9964 - dense_3_acc_2: 0.9148 - dense_3_acc_3: 0.9483 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9888 - dense_3_acc_6: 0.9226 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8932 - dense_3_acc_9: 0.8852\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 5s 509us/step - loss: 1.3440 - dense_3_loss: 0.3336 - dense_3_acc: 0.9977 - dense_3_acc_1: 0.9972 - dense_3_acc_2: 0.9180 - dense_3_acc_3: 0.9482 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9888 - dense_3_acc_6: 0.9239 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8965 - dense_3_acc_9: 0.8885\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 5s 491us/step - loss: 1.3102 - dense_3_loss: 0.3264 - dense_3_acc: 0.9975 - dense_3_acc_1: 0.9973 - dense_3_acc_2: 0.9228 - dense_3_acc_3: 0.9475 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9896 - dense_3_acc_6: 0.9274 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.8986 - dense_3_acc_9: 0.8879\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 5s 468us/step - loss: 1.2820 - dense_3_loss: 0.3202 - dense_3_acc: 0.9979 - dense_3_acc_1: 0.9979 - dense_3_acc_2: 0.9238 - dense_3_acc_3: 0.9471 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9893 - dense_3_acc_6: 0.9298 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9008 - dense_3_acc_9: 0.8900\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 5s 502us/step - loss: 1.2501 - dense_3_loss: 0.3144 - dense_3_acc: 0.9982 - dense_3_acc_1: 0.9980 - dense_3_acc_2: 0.9271 - dense_3_acc_3: 0.9485 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9897 - dense_3_acc_6: 0.9309 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9072 - dense_3_acc_9: 0.8914\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 5s 498us/step - loss: 1.2241 - dense_3_loss: 0.3086 - dense_3_acc: 0.9983 - dense_3_acc_1: 0.9981 - dense_3_acc_2: 0.9297 - dense_3_acc_3: 0.9484 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9896 - dense_3_acc_6: 0.9303 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9073 - dense_3_acc_9: 0.8933\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 5s 491us/step - loss: 1.1957 - dense_3_loss: 0.3026 - dense_3_acc: 0.9982 - dense_3_acc_1: 0.9982 - dense_3_acc_2: 0.9316 - dense_3_acc_3: 0.9488 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9896 - dense_3_acc_6: 0.9340 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9123 - dense_3_acc_9: 0.8937\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 5s 492us/step - loss: 1.1687 - dense_3_loss: 0.2962 - dense_3_acc: 0.9982 - dense_3_acc_1: 0.9986 - dense_3_acc_2: 0.9349 - dense_3_acc_3: 0.9489 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9898 - dense_3_acc_6: 0.9350 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9127 - dense_3_acc_9: 0.8957\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 5s 498us/step - loss: 1.1452 - dense_3_loss: 0.2929 - dense_3_acc: 0.9985 - dense_3_acc_1: 0.9985 - dense_3_acc_2: 0.9363 - dense_3_acc_3: 0.9490 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9900 - dense_3_acc_6: 0.9366 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9159 - dense_3_acc_9: 0.8982\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 5s 501us/step - loss: 1.1243 - dense_3_loss: 0.2889 - dense_3_acc: 0.9986 - dense_3_acc_1: 0.9988 - dense_3_acc_2: 0.9395 - dense_3_acc_3: 0.9503 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9896 - dense_3_acc_6: 0.9388 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9203 - dense_3_acc_9: 0.8994\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 5s 492us/step - loss: 1.0992 - dense_3_loss: 0.2837 - dense_3_acc: 0.9986 - dense_3_acc_1: 0.9987 - dense_3_acc_2: 0.9414 - dense_3_acc_3: 0.9497 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9904 - dense_3_acc_6: 0.9400 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9238 - dense_3_acc_9: 0.9000\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 5s 545us/step - loss: 1.0816 - dense_3_loss: 0.2798 - dense_3_acc: 0.9985 - dense_3_acc_1: 0.9991 - dense_3_acc_2: 0.9427 - dense_3_acc_3: 0.9517 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9903 - dense_3_acc_6: 0.9404 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9260 - dense_3_acc_9: 0.9013\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 6s 551us/step - loss: 1.0604 - dense_3_loss: 0.2747 - dense_3_acc: 0.9988 - dense_3_acc_1: 0.9989 - dense_3_acc_2: 0.9455 - dense_3_acc_3: 0.9524 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9906 - dense_3_acc_6: 0.9426 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9275 - dense_3_acc_9: 0.9022\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 5s 530us/step - loss: 1.0377 - dense_3_loss: 0.2703 - dense_3_acc: 0.9988 - dense_3_acc_1: 0.9994 - dense_3_acc_2: 0.9502 - dense_3_acc_3: 0.9540 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9905 - dense_3_acc_6: 0.9444 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9312 - dense_3_acc_9: 0.9035\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 5s 483us/step - loss: 1.0186 - dense_3_loss: 0.2671 - dense_3_acc: 0.9987 - dense_3_acc_1: 0.9994 - dense_3_acc_2: 0.9505 - dense_3_acc_3: 0.9540 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9909 - dense_3_acc_6: 0.9455 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9329 - dense_3_acc_9: 0.9035\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 5s 511us/step - loss: 1.0016 - dense_3_loss: 0.2632 - dense_3_acc: 0.9989 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9539 - dense_3_acc_3: 0.9560 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9903 - dense_3_acc_6: 0.9458 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9343 - dense_3_acc_9: 0.9052\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 5s 513us/step - loss: 0.9828 - dense_3_loss: 0.2596 - dense_3_acc: 0.9989 - dense_3_acc_1: 0.9995 - dense_3_acc_2: 0.9542 - dense_3_acc_3: 0.9553 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9911 - dense_3_acc_6: 0.9469 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9374 - dense_3_acc_9: 0.9064\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 5s 483us/step - loss: 0.9699 - dense_3_loss: 0.2585 - dense_3_acc: 0.9990 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9589 - dense_3_acc_3: 0.9570 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9909 - dense_3_acc_6: 0.9465 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9390 - dense_3_acc_9: 0.9073\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 5s 492us/step - loss: 0.9515 - dense_3_loss: 0.2541 - dense_3_acc: 0.9991 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9581 - dense_3_acc_3: 0.9573 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9915 - dense_3_acc_6: 0.9492 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9405 - dense_3_acc_9: 0.9073\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 5s 498us/step - loss: 0.9359 - dense_3_loss: 0.2496 - dense_3_acc: 0.9993 - dense_3_acc_1: 0.9998 - dense_3_acc_2: 0.9612 - dense_3_acc_3: 0.9579 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9910 - dense_3_acc_6: 0.9488 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9415 - dense_3_acc_9: 0.9106\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 548us/step - loss: 0.9190 - dense_3_loss: 0.2467 - dense_3_acc: 0.9990 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9636 - dense_3_acc_3: 0.9596 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9913 - dense_3_acc_6: 0.9510 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9433 - dense_3_acc_9: 0.9106\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 5s 502us/step - loss: 0.9040 - dense_3_loss: 0.2443 - dense_3_acc: 0.9993 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9658 - dense_3_acc_3: 0.9622 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9916 - dense_3_acc_6: 0.9504 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9449 - dense_3_acc_9: 0.9107\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 5s 492us/step - loss: 0.8887 - dense_3_loss: 0.2408 - dense_3_acc: 0.9992 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9675 - dense_3_acc_3: 0.9611 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9911 - dense_3_acc_6: 0.9518 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9471 - dense_3_acc_9: 0.9132\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 5s 487us/step - loss: 0.8733 - dense_3_loss: 0.2368 - dense_3_acc: 0.9993 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9699 - dense_3_acc_3: 0.9646 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9912 - dense_3_acc_6: 0.9522 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9475 - dense_3_acc_9: 0.9155\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 5s 495us/step - loss: 0.8616 - dense_3_loss: 0.2345 - dense_3_acc: 0.9993 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9692 - dense_3_acc_3: 0.9641 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9915 - dense_3_acc_6: 0.9527 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9494 - dense_3_acc_9: 0.9149\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 5s 513us/step - loss: 0.8488 - dense_3_loss: 0.2330 - dense_3_acc: 0.9993 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9716 - dense_3_acc_3: 0.9654 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9918 - dense_3_acc_6: 0.9527 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9496 - dense_3_acc_9: 0.9167\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 5s 536us/step - loss: 0.8352 - dense_3_loss: 0.2292 - dense_3_acc: 0.9995 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9736 - dense_3_acc_3: 0.9678 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9915 - dense_3_acc_6: 0.9546 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9507 - dense_3_acc_9: 0.9173\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 5s 548us/step - loss: 0.8235 - dense_3_loss: 0.2265 - dense_3_acc: 0.9994 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9740 - dense_3_acc_3: 0.9699 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9913 - dense_3_acc_6: 0.9553 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9511 - dense_3_acc_9: 0.9179\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 6s 551us/step - loss: 0.8088 - dense_3_loss: 0.2245 - dense_3_acc: 0.9994 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9770 - dense_3_acc_3: 0.9723 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9922 - dense_3_acc_6: 0.9557 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9505 - dense_3_acc_9: 0.9189\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 5s 526us/step - loss: 0.7993 - dense_3_loss: 0.2222 - dense_3_acc: 0.9994 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9788 - dense_3_acc_3: 0.9736 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9910 - dense_3_acc_6: 0.9548 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9559 - dense_3_acc_9: 0.9204\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 5s 497us/step - loss: 0.7833 - dense_3_loss: 0.2189 - dense_3_acc: 0.9995 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9799 - dense_3_acc_3: 0.9743 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9921 - dense_3_acc_6: 0.9562 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9558 - dense_3_acc_9: 0.9230\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 5s 480us/step - loss: 0.7718 - dense_3_loss: 0.2161 - dense_3_acc: 0.9995 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9809 - dense_3_acc_3: 0.9751 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9917 - dense_3_acc_6: 0.9571 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9572 - dense_3_acc_9: 0.9213\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 5s 517us/step - loss: 0.7593 - dense_3_loss: 0.2129 - dense_3_acc: 0.9997 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9814 - dense_3_acc_3: 0.9757 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9919 - dense_3_acc_6: 0.9584 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9579 - dense_3_acc_9: 0.9248\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 5s 525us/step - loss: 0.7470 - dense_3_loss: 0.2106 - dense_3_acc: 0.9997 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9839 - dense_3_acc_3: 0.9776 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9923 - dense_3_acc_6: 0.9586 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9604 - dense_3_acc_9: 0.9264\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 5s 503us/step - loss: 0.7367 - dense_3_loss: 0.2093 - dense_3_acc: 0.9997 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9852 - dense_3_acc_3: 0.9798 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9923 - dense_3_acc_6: 0.9588 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9597 - dense_3_acc_9: 0.9260\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 5s 508us/step - loss: 0.7274 - dense_3_loss: 0.2065 - dense_3_acc: 0.9993 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9861 - dense_3_acc_3: 0.9805 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9921 - dense_3_acc_6: 0.9584 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9596 - dense_3_acc_9: 0.9275\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 5s 487us/step - loss: 0.7177 - dense_3_loss: 0.2043 - dense_3_acc: 0.9997 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9863 - dense_3_acc_3: 0.9818 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9922 - dense_3_acc_6: 0.9588 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9624 - dense_3_acc_9: 0.9294\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 5s 501us/step - loss: 0.7054 - dense_3_loss: 0.2011 - dense_3_acc: 0.9996 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9872 - dense_3_acc_3: 0.9833 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9924 - dense_3_acc_6: 0.9602 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9628 - dense_3_acc_9: 0.9310\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 5s 535us/step - loss: 0.6968 - dense_3_loss: 0.1995 - dense_3_acc: 0.9996 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 0.9888 - dense_3_acc_3: 0.9840 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9920 - dense_3_acc_6: 0.9607 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9631 - dense_3_acc_9: 0.9304\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 5s 498us/step - loss: 0.6859 - dense_3_loss: 0.1977 - dense_3_acc: 0.9998 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9891 - dense_3_acc_3: 0.9850 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9923 - dense_3_acc_6: 0.9601 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9636 - dense_3_acc_9: 0.9328\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 5s 511us/step - loss: 0.6770 - dense_3_loss: 0.1956 - dense_3_acc: 0.9998 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9902 - dense_3_acc_3: 0.9864 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9923 - dense_3_acc_6: 0.9617 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9666 - dense_3_acc_9: 0.9339\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 5s 493us/step - loss: 0.6665 - dense_3_loss: 0.1920 - dense_3_acc: 0.9997 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9905 - dense_3_acc_3: 0.9876 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9923 - dense_3_acc_6: 0.9609 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9651 - dense_3_acc_9: 0.9339\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 5s 520us/step - loss: 0.6580 - dense_3_loss: 0.1908 - dense_3_acc: 0.9997 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9910 - dense_3_acc_3: 0.9877 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9924 - dense_3_acc_6: 0.9618 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9659 - dense_3_acc_9: 0.9369\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 508us/step - loss: 0.6464 - dense_3_loss: 0.1876 - dense_3_acc: 0.9998 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9911 - dense_3_acc_3: 0.9892 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9925 - dense_3_acc_6: 0.9625 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9677 - dense_3_acc_9: 0.9376\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 5s 487us/step - loss: 0.6382 - dense_3_loss: 0.1859 - dense_3_acc: 0.9998 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9927 - dense_3_acc_3: 0.9895 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9926 - dense_3_acc_6: 0.9638 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9679 - dense_3_acc_9: 0.9387\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 5s 489us/step - loss: 0.6284 - dense_3_loss: 0.1837 - dense_3_acc: 0.9997 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9924 - dense_3_acc_3: 0.9904 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9929 - dense_3_acc_6: 0.9626 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9683 - dense_3_acc_9: 0.9406\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 5s 493us/step - loss: 0.6200 - dense_3_loss: 0.1818 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9937 - dense_3_acc_3: 0.9901 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9926 - dense_3_acc_6: 0.9636 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9701 - dense_3_acc_9: 0.9408\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 5s 480us/step - loss: 0.6126 - dense_3_loss: 0.1793 - dense_3_acc: 0.9998 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9943 - dense_3_acc_3: 0.9910 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9919 - dense_3_acc_6: 0.9629 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9702 - dense_3_acc_9: 0.9417\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 5s 495us/step - loss: 0.6022 - dense_3_loss: 0.1770 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9942 - dense_3_acc_3: 0.9913 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9930 - dense_3_acc_6: 0.9648 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9703 - dense_3_acc_9: 0.9427\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 5s 498us/step - loss: 0.5942 - dense_3_loss: 0.1753 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9945 - dense_3_acc_3: 0.9927 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9926 - dense_3_acc_6: 0.9652 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9715 - dense_3_acc_9: 0.9447\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 5s 535us/step - loss: 0.5869 - dense_3_loss: 0.1730 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9947 - dense_3_acc_3: 0.9918 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9925 - dense_3_acc_6: 0.9638 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9720 - dense_3_acc_9: 0.9449\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 5s 529us/step - loss: 0.5791 - dense_3_loss: 0.1715 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9954 - dense_3_acc_3: 0.9924 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9927 - dense_3_acc_6: 0.9652 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9723 - dense_3_acc_9: 0.9463\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 5s 531us/step - loss: 0.5728 - dense_3_loss: 0.1702 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9956 - dense_3_acc_3: 0.9931 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9931 - dense_3_acc_6: 0.9645 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9725 - dense_3_acc_9: 0.9463\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 5s 502us/step - loss: 0.5629 - dense_3_loss: 0.1670 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9951 - dense_3_acc_3: 0.9940 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9927 - dense_3_acc_6: 0.9666 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9732 - dense_3_acc_9: 0.9476\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 5s 508us/step - loss: 0.5577 - dense_3_loss: 0.1660 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9956 - dense_3_acc_3: 0.9941 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9925 - dense_3_acc_6: 0.9654 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9739 - dense_3_acc_9: 0.9476\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 5s 499us/step - loss: 0.5522 - dense_3_loss: 0.1649 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9958 - dense_3_acc_3: 0.9951 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9923 - dense_3_acc_6: 0.9656 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9736 - dense_3_acc_9: 0.9481\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 5s 507us/step - loss: 0.5437 - dense_3_loss: 0.1620 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9961 - dense_3_acc_3: 0.9956 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9931 - dense_3_acc_6: 0.9660 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9747 - dense_3_acc_9: 0.9503\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 5s 509us/step - loss: 0.5371 - dense_3_loss: 0.1607 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9966 - dense_3_acc_3: 0.9952 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9931 - dense_3_acc_6: 0.9664 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9762 - dense_3_acc_9: 0.9504\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 5s 462us/step - loss: 0.5293 - dense_3_loss: 0.1584 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9969 - dense_3_acc_3: 0.9954 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9922 - dense_3_acc_6: 0.9668 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9759 - dense_3_acc_9: 0.9510\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 5s 450us/step - loss: 0.5231 - dense_3_loss: 0.1572 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9967 - dense_3_acc_3: 0.9956 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9928 - dense_3_acc_6: 0.9666 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9764 - dense_3_acc_9: 0.9516\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 5s 452us/step - loss: 0.5182 - dense_3_loss: 0.1558 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9971 - dense_3_acc_3: 0.9966 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9930 - dense_3_acc_6: 0.9666 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9778 - dense_3_acc_9: 0.9530\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 5s 509us/step - loss: 0.5106 - dense_3_loss: 0.1538 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9974 - dense_3_acc_3: 0.9968 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9930 - dense_3_acc_6: 0.9667 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9769 - dense_3_acc_9: 0.9538\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 5s 523us/step - loss: 0.5066 - dense_3_loss: 0.1530 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9977 - dense_3_acc_3: 0.9963 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9930 - dense_3_acc_6: 0.9679 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9763 - dense_3_acc_9: 0.9523\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 5s 498us/step - loss: 0.4979 - dense_3_loss: 0.1499 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9977 - dense_3_acc_3: 0.9973 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9932 - dense_3_acc_6: 0.9687 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9756 - dense_3_acc_9: 0.9552\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 5s 516us/step - loss: 0.4932 - dense_3_loss: 0.1487 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9980 - dense_3_acc_3: 0.9968 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9930 - dense_3_acc_6: 0.9691 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9783 - dense_3_acc_9: 0.9571\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 5s 493us/step - loss: 0.4868 - dense_3_loss: 0.1467 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9980 - dense_3_acc_3: 0.9974 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9934 - dense_3_acc_6: 0.9684 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9784 - dense_3_acc_9: 0.9578\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 477us/step - loss: 0.4811 - dense_3_loss: 0.1455 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9980 - dense_3_acc_3: 0.9974 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9933 - dense_3_acc_6: 0.9674 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9784 - dense_3_acc_9: 0.9572\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 5s 516us/step - loss: 0.4770 - dense_3_loss: 0.1448 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9982 - dense_3_acc_3: 0.9977 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9931 - dense_3_acc_6: 0.9692 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9790 - dense_3_acc_9: 0.9562\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 5s 471us/step - loss: 0.4698 - dense_3_loss: 0.1421 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9979 - dense_3_acc_3: 0.9979 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9934 - dense_3_acc_6: 0.9687 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9786 - dense_3_acc_9: 0.9579\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 5s 468us/step - loss: 0.4647 - dense_3_loss: 0.1405 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9986 - dense_3_acc_3: 0.9979 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9936 - dense_3_acc_6: 0.9677 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9800 - dense_3_acc_9: 0.9589\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 5s 489us/step - loss: 0.4593 - dense_3_loss: 0.1391 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9986 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9929 - dense_3_acc_6: 0.9695 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9799 - dense_3_acc_9: 0.9590\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 5s 505us/step - loss: 0.4552 - dense_3_loss: 0.1378 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9985 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9935 - dense_3_acc_6: 0.9691 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9805 - dense_3_acc_9: 0.9607\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 5s 499us/step - loss: 0.4486 - dense_3_loss: 0.1359 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9987 - dense_3_acc_3: 0.9985 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9934 - dense_3_acc_6: 0.9700 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9811 - dense_3_acc_9: 0.9618\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 5s 539us/step - loss: 0.4462 - dense_3_loss: 0.1352 - dense_3_acc: 0.9999 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9985 - dense_3_acc_3: 0.9986 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9937 - dense_3_acc_6: 0.9700 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9821 - dense_3_acc_9: 0.9611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa6c4f41630>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 37)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xoh.shape\n",
    "Xoh[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "source: 5 April 09\n",
      "output: 2009-04-04\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-20\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    source = np.transpose(source).reshape(1, 30,37)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
